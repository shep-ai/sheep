# Sheep - Environment Configuration
# Copy this file to .env and fill in your values

# =============================================================================
# LLM Provider API Keys (add the ones you want to use)
# =============================================================================

# OpenAI
OPENAI_API_KEY=sk-...

# Anthropic (Claude)
ANTHROPIC_API_KEY=sk-ant-...

# Google (Gemini)
GOOGLE_API_KEY=...
# Or use Vertex AI
# GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json
# VERTEXAI_PROJECT=your-project-id
# VERTEXAI_LOCATION=us-central1

# Cursor API (if available)
CURSOR_API_KEY=...
CURSOR_API_BASE=https://api.cursor.sh/v1

# =============================================================================
# Default Model Configuration
# =============================================================================

# Default model to use (format: provider/model-name)
# Examples: openai/gpt-4o, anthropic/claude-3-5-sonnet, gemini/gemini-2.5-flash
SHEEP_DEFAULT_MODEL=openai/gpt-4o

# Model for fast/cheap operations (research, simple tasks)
# Recommended: openai/gpt-4o-mini, gemini/gemini-2.5-flash, gemini/gemini-2.0-flash-lite
SHEEP_FAST_MODEL=openai/gpt-4o-mini

# Model for complex reasoning (implementation, code review)
# Recommended: anthropic/claude-3-5-sonnet-20241022, gemini/gemini-3-pro-preview, openai/gpt-4o
# Note: Gemini 3 models are preview and may be unstable - use gemini/gemini-2.5-flash for stability
SHEEP_REASONING_MODEL=anthropic/claude-3-5-sonnet-20241022

# =============================================================================
# Observability - Langfuse
# =============================================================================

LANGFUSE_PUBLIC_KEY=pk-lf-...
LANGFUSE_SECRET_KEY=sk-lf-...
# Use LANGFUSE_BASE_URL (recommended) or LANGFUSE_HOST (legacy, both work)
# For Langfuse Cloud EU: https://cloud.langfuse.com
# For Langfuse Cloud US: https://us.cloud.langfuse.com
# For self-hosted: http://localhost:3000
LANGFUSE_BASE_URL=https://cloud.langfuse.com
# Set to false to disable tracing
LANGFUSE_ENABLED=true

# OpenInference CrewAI instrumentation - captures detailed agent steps, tool calls, and LLM generations
# When enabled, you'll see individual spans in Langfuse for each agent iteration, tool call, etc.
# When disabled, you'll only see high-level flow traces with concatenated output
# Note: Requires 'pip install openinference-instrumentation-crewai'
# Recommendation: Enable this (true) for detailed observability
LANGFUSE_OPENLIT_ENABLED=true

# =============================================================================
# Git Configuration
# =============================================================================

# Default git remote name
SHEEP_GIT_REMOTE=origin

# Branch prefix for auto-generated branches
SHEEP_BRANCH_PREFIX=sheep/

# =============================================================================
# Execution Settings
# =============================================================================

# Log level: DEBUG, INFO, WARNING, ERROR
SHEEP_LOG_LEVEL=INFO

# Enable verbose agent output
SHEEP_VERBOSE=false

# Max iterations for agent loops (prevents runaway execution)
SHEEP_MAX_ITERATIONS=25
